import sys
import argparse
import numpy as np
import keras
import random
import gym
import matplotlib.pyplot as plt

class Imitation():
    def __init__(self, model_config_path, expert_weights_path):
        # Load the expert model.
        with open(model_config_path, 'r') as f:
            self.expert = keras.models.model_from_json(f.read())
        self.expert.load_weights(expert_weights_path)

        # Arguments are required for compilation, but not used for expert
        # self.expert.compile(optimizer='Adam', loss='categorical_crossentropy') 
        
        # Initialize the cloned model (to be trained).
        with open(model_config_path, 'r') as f:
            self.model = keras.models.model_from_json(f.read())

        # TODO: Delete once done troubleshooting, weights appear to be correct
        # print(self.model.get_weights())

        # TODO: Define any training operations and optimizers here, initialize
        #       your variables, or alternatively compile your model here.

        # TODO: Arguments specified in pdf.  There are multiple crossentropy functions in Keras, however
        self.model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy']) 

        # TODO: Delete this once no longer in use, taken from env.reset()
        # test_state = np.array([[0.0061801, 0.93431817, 0.62596564, -0.41969598, -0.00715443, -0.1417985, 0.0, 0.0]])
        # print(test_state.shape)
        # print(self.model.predict(x=test_state, verbose=1))
        # raw_input()


    def run_expert(self, env, render=False):
        # Generates an episode by running the expert policy on the given env.
        return Imitation.generate_episode(self.expert, env, render)

    def run_model(self, env, render=False):
        # Generates an episode by running the cloned policy on the given env.
        return Imitation.generate_episode(self.model, env, render)

    @staticmethod
    def generate_episode(model, env, render=False):
        # Generates an episode by running the given model on the given env.
        # Returns:
        # - a list of states, indexed by time step
        # - a list of actions, indexed by time step
        # - a list of rewards, indexed by time step
        # TODO: Implement this method.

        states = []
        actions = []
        rewards = []
        num_episodes = 1  # TODO: 10, 50, and 100 episodes

        for episode in range(num_episodes):

        	# TODO: Create first index by episode?  Seems reasonable
        	e_states = []
        	e_actions = []
        	e_rewards = []

        	done = False
        	state = env.reset()  # Restart the environment
        	while not done:  
        		e_states.append(state)  # TODO: Should this be done before or after reshape?
        		state = np.array([state])
        		model_output = model.predict(x = state, verbose = 0)  # Get action from model
        		action = np.argmax(model_output)  # Equivalent to greedy policy
        		e_actions.append(action)
        		state, reward, done, info = env.step(action)
        		e_rewards.append(reward)
        		if render:
        			env.render()

        	# Add episode to list
        	states.append(e_states)
        	actions.append(e_actions)
        	rewards.append(e_rewards)


        return states, actions, rewards
    
    def train(self, env, num_episodes=100, num_epochs=50, render=False):
        # Trains the model on training data generated by the expert policy.
        # Args:
        # - env: The environment to run the expert policy on. 
        # - num_episodes: # episodes to be generated by the expert.
        # - num_epochs: # epochs to train on the data generated by the expert.
        # - render: Whether to render the environment.
        # Returns the final loss and accuracy.
        # TODO: Implement this method. It may be helpful to call the class
        #       method run_expert() to generate training data.
        loss = 0
        acc = 0
        return loss, acc


def parse_arguments():
    # Command-line flags are defined here.
    parser = argparse.ArgumentParser()
    parser.add_argument('--model-config-path', dest='model_config_path',
                        type=str, default='LunarLander-v2-config.json',
                        help="Path to the model config file.")
    parser.add_argument('--expert-weights-path', dest='expert_weights_path',
                        type=str, default='LunarLander-v2-weights.h5',
                        help="Path to the expert weights file.")

    # https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
    parser_group = parser.add_mutually_exclusive_group(required=False)
    parser_group.add_argument('--render', dest='render',
                              action='store_true',
                              help="Whether to render the environment.")
    parser_group.add_argument('--no-render', dest='render',
                              action='store_false',
                              help="Whether to render the environment.")
    parser.set_defaults(render=False)

    return parser.parse_args()


def main(args):
    # Parse command-line arguments.
    args = parse_arguments()
    model_config_path = args.model_config_path
    expert_weights_path = args.expert_weights_path
    render = args.render
    
    # Create the environment.
    env = gym.make('LunarLander-v2')
    
    # TODO: Train cloned models using imitation learning, and record their
    #       performance.
    imitation = Imitation(model_config_path, expert_weights_path)
    
    # states, actions, rewards = imitation.generate_episode(imitation.expert, env, render)

    # TODO: Delete this for submission.  Toggle to check that model crashes w/o training
    states, actions, rewards = imitation.generate_episode(imitation.model, env, render)
    print(rewards)
    raw_input("Did it crash? (ends in -100)")



if __name__ == '__main__':
  main(sys.argv)

# import sys
# import argparse
# import numpy as np
# import keras
# import random
# import gym
# import matplotlib.pyplot as plt
# import pdb

# class Imitation():
# 	def __init__(self, model_config_path, expert_weights_path):

# 		# Load the expert model.
# 		with open(model_config_path, 'r') as f:
# 			self.expert = keras.models.model_from_json(f.read())
			
# 		self.expert.load_weights(expert_weights_path)

# 		# Initialize the cloned model (to be trained).
# 		with open(model_config_path, 'r') as f:
# 			self.model = keras.models.model_from_json(f.read())

# 		# TODO: Define any training operations and optimizers here, initialize
# 		#       your variables, or alternatively compile your model here.

# 		# TODO: Arguments specified in pdf.  There are multiple crossentropy functions in Keras, however
# 		self.model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy']) 

# 	def run_expert(self, env, num_episodes=1, render=False):
# 		# Generates an episode by running the expert policy on the given env.
# 		return Imitation.generate_episode(self.expert, env, False, num_episodes)

# 	def run_model(self, env, num_episodes=1, render=False):
# 		# Generates an episode by running the cloned policy on the given env.
# 		return Imitation.generate_episode(self.model, env, render, num_episodes)

# 	@staticmethod
# 	def generate_episode(model, env, render=False, num_episodes=1):
# 		# Generates an episode by running the given model on the given env.
# 		# Returns:
# 		# - a list of states, indexed by time step
# 		# - a list of actions, indexed by time step
# 		# - a list of rewards, indexed by time step
# 		# TODO: Implement this method.

# 		states = []
# 		actions = []
# 		rewards = []
# 		num_episodes = num_episodes  # TODO: 10, 50, and 100 episodes
# 		e_states = []
# 		e_actions = []
# 		e_rewards = []

# 		for episode in range(num_episodes):

# 			# TODO: Create first index by episode?  Seems reasonable


# 			done = False
# 			state = env.reset()  # Restart the environment
# 			while not done:  
# 				e_states.append(state)  # TODO: Should this be done before or after reshape?
# 				state = np.array([state])
# 				model_output = model.predict(x = state, verbose = 0)  # Get action from model
# 				action = np.zeros_like(model_output)
# 				max_action = np.argmax(model_output)
# 				action[0, max_action] = 1
# 				e_actions.append(action)
# 				state, reward, done, info = env.step(max_action)
# 				e_rewards.append(reward)
# 				if render:
# 					env.render()

# 			# Add episode to list
# 			states.append(e_states)
# 			actions.append(e_actions)
# 			rewards.append(e_rewards)


# 		return np.array(states), np.array(actions), np.array(rewards)
	
# 	def train(self, env, num_episodes=100, num_epochs=50, render=False):
# 		# Trains the model on training data generated by the expert policy.
# 		# Args:
# 		# - env: The environment to run the expert policy on. 
# 		# - num_episodes: # episodes to be generated by the expert.
# 		# - num_epochs: # epochs to train on the data generated by the expert.
# 		# - render: Whether to render the environment.
# 		# Returns the final loss and accuracy.
# 		# TODO: Implement this method. It may be helpful to call the class
# 		#       method run_expert() to generate training data.
# 		loss = 0
# 		acc = 0
		
# 		expert_states, expert_actions, expert_rewards = self.run_expert(env, num_episodes, render)
# 		# expert_states = expert_states.swapaxes(0,2)
# 		# expert_actions = expert_actions.swapaxes(0,2)
# 		#
# 		# expert_states = expert_states.swapaxes(0, 1)
# 		# expert_actions = expert_actions.swapaxes(0, 1)
# 		# pdb.set_trace()
# 		# expert_states = expert_states.flatten()
# 		# expert_actions = expert_actions.flatten()

# 		# for i in range(expert_states.shape[2]):
# 		#   expert_states[:,:,0]

# 		# expert_states = np.squeeze(expert_states)
# 		# expert_actions = np.squeeze(expert_actions)
# 		# pdb.set_trace()

# 		expert_states = np.transpose(expert_states, (2,1,0))
# 		expert_actions = np.transpose(expert_actions[:,:,0], (2,1,0))

# 		# pdb.set_trace()

# 		expert_states = np.reshape(expert_states, (8,-1))
# 		expert_actions = np.reshape(expert_actions, (env.action_space.n,-1))
# 		# expert_rewards = np.reshape(expert_rewards, (1,))

# 		expert_states = np.transpose(expert_states, (1, 0))
# 		expert_actions = np.transpose(expert_actions, (1, 0))

# 		# pdb.set_trace()
# 		history = self.model.fit(expert_states, expert_actions, batch_size=32, nb_epoch=num_epochs, verbose=1)

# 		loss = sum(history.history['loss'])/float(len(history.history['loss']))
# 		acc = sum(history.history['acc'])/float(len(history.history['acc']))
# 		# s = env.reset()

# 		# for e in num_episodes:
# 		#   for i in range(len(expert_actions)):
# 		#       self.model.fit(s, expert_actions[i], batch_size=None, nb_epoch=num_epochs, verbose=1)
# 		#       output = self.model.predict(x, batch_size=None, verbose=0, steps=None)
# 		#       s, _, _, _ = env.step(np.argmax(output))
# 		print("{} are {}, {}".format(self.model.metrics_names, loss, acc))
		
# 		return loss, acc

# def parse_arguments():
# 	# Command-line flags are defined here.
# 	parser = argparse.ArgumentParser()
# 	parser.add_argument('--model-config-path', dest='model_config_path',
# 						type=str, default='LunarLander-v2-config.json',
# 						help="Path to the model config file.")
# 	parser.add_argument('--expert-weights-path', dest='expert_weights_path',
# 						type=str, default='LunarLander-v2-weights.h5',
# 						help="Path to the expert weights file.")

# 	# https://stackoverflow.com/questions/15008758/parsing-boolean-values-with-argparse
# 	parser_group = parser.add_mutually_exclusive_group(required=False)
# 	parser_group.add_argument('--render', dest='render',
# 							  action='store_true',
# 							  help="Whether to render the environment.")
# 	parser_group.add_argument('--no-render', dest='render',
# 							  action='store_false',
# 							  help="Whether to render the environment.")
# 	parser.set_defaults(render=False)

# 	return parser.parse_args()


# def main(args):

#     # Parse command-line arguments.
#     args = parse_arguments()
#     model_config_path = args.model_config_path
#     expert_weights_path = args.expert_weights_path
#     render = args.render
    
#     # Create the environment.
#     env = gym.make('LunarLander-v2')
    
#     # TODO: Train cloned models using imitation learning, and record their
#     #       performance.

#     num_episodes = 1
#     num_epochs = 50

#     imitation = Imitation(model_config_path, expert_weights_path)
#     loss, acc = imitation.train(env, num_episodes, num_epochs, render=render)

#     save_path = "imititation-weights-" + str(num_episodes) + "-" + str(num_epochs) + ".h5"
#     imitation.model.save_weights(save_path)

#     # For generating videos
#     NUM_DEMOS = 50 # Do not change, this is a fixed number
#     demo_rewards = []
#     demo_render = True
#     raw_input("Press any key to run demos and record rewards")
#     for test in range(NUM_DEMOS):
#         _, _, rewards = imitation.run_model(env, render=demo_render)
#         reward_sum = np.sum(rewards)
#         demo_rewards.append(reward_sum)
#     print("Mean of demos is: {}".format(np.mean(demo_rewards)))
#     print("Std of demos is {}".format(np.std(demo_rewards)))


# if __name__ == '__main__':
#   main(sys.argv)
